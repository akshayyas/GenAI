# -*- coding: utf-8 -*-
"""Fast Food Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U19UMXLtYvz3hUkrICBsbimVatfLn6ZB
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'fast-food-classification-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2631527%2F4666824%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240329%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240329T165122Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D329d9e1ee15ef7bacf3edfa1db87041f968a9911499fcf56fb10b9e9884a3188a8d34e5f21921fec160ad2c79b70a6f1888a9e42dd8b1c2724300bed60b0f9bea57b036eff69322c8ab297325be10bf57476a50d8355f7583f79e1b0031783e44e6d290e32c824cc2f61901588d3aa7b25b3f657df72d0d84702d96161e5c0c11913df66342ad2be4dbf5f80a4a10fb237a79493ea9778eb0b26e1649d57b3ea85cd3ad85d52490d9356742293810a22866f7c8fa5cb84e0f529b7b51a41ecd4b5d0b2fc15d5a6d6372584591535f5f36724d113fbf34a61ec90a3320ed838f893a8b6bc0775dcae8cb1e8a3e2361d5aedf92da7888acfa3135d2a114368c00e,fast-food-classifier:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4629237%2F7910134%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240329%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240329T165122Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4daba4fc696e3932c424a03dad2f08e917d9150a78242ca7b20df4072fb6737b293737e5ba871be9d68b3dda5b7cfefac7e890d83668f01fad326b14ebbbf04ef13026e774822c46a9fb12e0be17e5d3c3fb0645916fb5641bf4c56e8e8a052390f89add093a41942788cf360cfeede6d257d56134cffb3d15991d14fa3695b9f2be19d3d8560087d5c163a9e4b83d3b886236684acb3345e34411c5502e7408d0de37e4ff25a8a9514525a61a04a5b4b7c5b8e889b0147516b543b3631ee98d93c48e2c79039bb22957af190da225481a8d6e6904348e572045117ecf4b8ae872f27cadd91d2d60d1383cbd7b38fa5eaaaa9264329e8eb44a08070a3a698bb0'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""**ENVIRONMENTAL SETUP**"""

!pip install -q tensorflowjs

# Main libraries
import os
import numpy as np
import pandas as pd
import tensorflow as tf

# Data Loading & Processing
import sklearn
from glob import glob
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Data Visualization
import plotly
import matplotlib
import plotly.express as px
import matplotlib.pyplot as plt

# NN
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import applications

print(f"{'Numpy Version':20}: {np.__version__}")
print(f"{'Pandas Version':20}: {pd.__version__}")
print(f"{'Tensorflow Version':20}: {tf.__version__}")
print(f"{'Plotly Version':20}: {plotly.__version__}")
print(f"{'Scikit-Learn Version':20}: {sklearn.__version__}")
print(f"{'Matplotlib Version':20}: {matplotlib.__version__}")

# Set computation engine as Tessorflow
os.environ["KERAS_BACKEND"] = "tensorflow"

# Data Configurations
BATCH_SIZE = 64
RANDOM_SEED = 42
STEPS_PER_EPOCH = 10
IMAGE_SIZE = (224, 224)

# Data Directory Paths
TRAIN_PATH = '/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Train'
VALID_PATH = '/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Valid'
TEST_PATH = '/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Test'

# Reprocducibility
tf.random.set_seed(RANDOM_SEED)

"""# **Fast Food Dataset**"""

# Collect the class names
class_names = sorted(list(map(lambda x: x.title(), os.listdir(TRAIN_PATH))))

# Compuet the total number of classes
n_classes = len(class_names)

for index, name in enumerate(class_names):
    print(f"{index+1:3} : {name.title()} ")

# Compute the number of images per class
n_images_per_class = [len(os.listdir(TRAIN_PATH + f"/{class_name}")) for class_name in class_names]

# Bar graph visualization
bar_graph = px.bar(x=class_names, y=n_images_per_class, title='Class Distribution', color=class_names)
bar_graph.update_layout(
    xaxis_title="Fast Food",
    yaxis_title="No. of Images",
    showlegend=False
)
bar_graph.show()

# Image Data Loader & Augmenter
train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
)
valid_gen = ImageDataGenerator(rescale=1./255)
test_gen = ImageDataGenerator(rescale=1./255)

# Training data
train_ds = train_gen.flow_from_directory(
    TRAIN_PATH,
    class_mode='binary',
    shuffle=True,
    seed=RANDOM_SEED,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE
)

valid_ds = train_gen.flow_from_directory(
    VALID_PATH,
    class_mode='binary',
    shuffle=False,
    seed=RANDOM_SEED,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE
)

# Testing data
test_ds = test_gen.flow_from_directory(
    TEST_PATH,
    class_mode='binary',
    seed=RANDOM_SEED,
    shuffle=True,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE
)

# Output Representation
for images, labels in train_ds:

    class_index = int(labels[0])
    one_hot = tf.one_hot(class_index, depth=n_classes).numpy()
    print(f"{'Class Index':20}: {class_index}")
    print(f"{'Class Name':20}: {class_names[class_index]}")
    print(f"{'One Hot':20}: {one_hot}")
    break

"""# **Data Visualization**"""

# Collect the Directory parts of each class.
dirs = [f"{TRAIN_PATH}/{name}/" for name in class_names]

# Select an image at random from each directory
image_paths = [np.random.choice(glob(directory + '/*')) for directory in dirs]

print("Image Paths:")
image_paths[:5]

def load_image(image_path):
    """
    Load and preprocess an image from the given file path.

    Parameters:
    - image_path (str): File path of the image.

    Returns:
    - image (tf.Tensor): Processed image tensor.
    """
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, IMAGE_SIZE)
    image = image/255.
    return image

def show_image(image):
    """
    Display the given image without axis.

    Parameters:
    - image (tf.Tensor): Image tensor to display.
    """
    plt.imshow(image)
    plt.axis('off')

def show_images(data, n_rows, n_cols, figsize=(15,15), model=None, image_paths=False, delimeter="\\"):
    """
    Display a grid of images from the given data.

    Parameters:
    - data (tf.data.Dataset or list): Dataset containing images and labels or list of image file paths.
    - n_rows (int): Number of rows in the display grid.
    - n_cols (int): Number of columns in the display grid.
    - figsize (tuple, optional): Figure size for matplotlib. Default is (15, 15).
    - model (tf.keras.Model, optional): Trained model for making predictions. Default is None.
    - image_paths (bool, optional): True if the data is a list of image file paths. Default is False.
    """
    plt.figure(figsize=figsize)

    if image_paths:
        for index, path in enumerate(data):
            image = load_image(path)
            label = path.split(delimeter)[-2].title()

            plt.subplot(n_rows, n_cols, index+1)
            show_image(image)
            plt.title(label)

    else:
        iterator = iter(data)
        for index in range(n_rows * n_cols):
            try:
                images, labels = next(iterator)
            except StopIteration:
                break

            if model is not None:
                model_pred = model.predict(images, verbose=0)
                model_pred = tf.argmax(model_pred, axis=-1)
                pred_labels = [class_names[index] for index in model_pred]

            for i in range(images.shape[0]):

                if i>=(n_rows * n_cols):
                    break

                plt.subplot(n_rows, n_cols, i + 1)
                show_image(tf.squeeze(images[i]))
                title = class_names[int(labels[i])]

                if model is not None:
                    title = f"T: {title}\nP: {pred_labels[i]}"

                plt.title(title)

    plt.show()

show_images(image_paths, 10, 5, figsize=(15, 30), image_paths=True, delimeter=".")

show_images(train_ds, 5, 5)

show_images(train_ds, 5, 5)
show_images(train_ds, 5, 5)
show_images(train_ds, 5, 5)
show_images(train_ds, 5, 5)

"""# **ResNet50V2**"""

# Defininig the backbone
backbone = keras.applications.ResNet50V2(input_shape=(*IMAGE_SIZE,3), include_top=False)
backbone.trainable = True

# Full Model : Backbone + Classidier
model = keras.Sequential([
    layers.Input(shape=(*IMAGE_SIZE, 3)),
    backbone,
    layers.BatchNormalization(),
    layers.Dropout(0.4),
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(n_classes, activation='softmax')
])

# Model Compilation
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    metrics=['accuracy']
)

# Define LR Scheduler
lr_scheduler = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',                              # Monitor validation loss
    factor=0.2,                                      # Factor by which learning rate will be reduced (new_lr = lr * factor)
    patience=3,                                      # Number of epochs with no improvement after which learning rate will be reduced
    min_lr=1e-6,                                     # Lower bound on the learning rate
    verbose=1                                        # Verbosity mode
)

# Training the model
history = model.fit(
    train_ds,
    epochs=100,
    validation_data=valid_ds,
    callbacks=[
        keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
        lr_scheduler,
        keras.callbacks.ModelCheckpoint(
            "FastFood-ResNet.keras",
            monitor="val_accuracy",
            save_best_only=True
        )
    ]
)

# Learning Curve
history_df = pd.DataFrame(history.history)
plt.figure(figsize=(15, 5))

# Loss Curve
plt.subplot(1,2,1)
plt.title("Loss Curve")
plt.plot(history_df['loss'], label="Loss")
plt.plot(history_df['val_loss'], label="Val Loss")
plt.grid()

# Accuracy Curve
plt.subplot(1,2,2)
plt.title("Accuracy Curve")
plt.plot(history_df['accuracy'], label="Accuracy")
plt.plot(history_df['val_accuracy'], label="Val Accuracy")
plt.grid()

# Plot Configs
plt.legend()
plt.savefig("LearningCurve.png")
plt.show()

# img = plt.imread(LC_PATH)
# plt.figure(figsize=(15, 10))
# plt.imshow(img)
# plt.axis('off')
# plt.show()

"""# **Tensorflow JS**"""

!tensorflowjs_converter --input_format keras {MODEL_PATH} './Model'
!zip -r Model.zip Model